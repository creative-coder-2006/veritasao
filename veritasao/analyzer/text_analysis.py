# analyzer/text_analysis.py

"""
VERITAS.AI Text Analysis Module
===============================

This module provides the core text analysis capabilities for VERITAS.AI,
focusing on two critical areas:
1.  Misinformation Detection: Using a fine-tuned BERT model to assess the
    likelihood of a text containing false or misleading information.
2.  LLM Origin Detection: Determining the probability that a text was
    generated by an AI language model.

It uses an ensemble approach for LLM detection, combining statistical methods
with a dedicated transformer-based model for robustness.
"""

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    pipeline
)
import numpy as np
from scipy.stats import entropy
from config import TEXT_ANALYZER_CONFIG
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TextAnalyzer:
    """
    Performs misinformation and LLM origin analysis on text data.
    """
    def __init__(self, config=TEXT_ANALYZER_CONFIG):
        """
        Initializes the TextAnalyzer by loading the necessary models and tokenizers.
        
        Args:
            config (dict): Configuration dictionary for the analyzer.
        """
        self.config = config
        self.device = 0 if torch.cuda.is_available() else -1
        logger.info(f"TextAnalyzer initialized on device: {'GPU' if self.device == 0 else 'CPU'}")

        # --- Load Misinformation Detection Model ---
        self.misinfo_tokenizer, self.misinfo_model = self._load_model_and_tokenizer(
            self.config.get("misinfo_model_name"),
            self.config.get("fallback_misinfo_model")
        )

        # --- Load LLM Origin Detection Model ---
        # Using a pipeline for the AI text detection model for simplicity
        try:
            llm_model_name = self.config.get("llm_origin_model_name")
            logger.info(f"Loading LLM origin detection model: {llm_model_name}")
            self.llm_detection_pipeline = pipeline(
                "text-classification",
                model=llm_model_name,
                tokenizer=llm_model_name,
                device=self.device
            )
        except Exception as e:
            logger.warning(
                f"Failed to load primary LLM detection model '{llm_model_name}': {e}. "
                f"Falling back to statistical analysis only."
            )
            self.llm_detection_pipeline = None

    def _load_model_and_tokenizer(self, primary_model_name, fallback_model_name):
        """
        Loads a model and tokenizer with a fallback mechanism.
        """
        try:
            logger.info(f"Loading model and tokenizer: {primary_model_name}")
            tokenizer = AutoTokenizer.from_pretrained(primary_model_name)
            model = AutoModelForSequenceClassification.from_pretrained(primary_model_name)
            model.to("cuda" if self.device == 0 else "cpu")
            return tokenizer, model
        except Exception as e:
            logger.warning(
                f"Failed to load model '{primary_model_name}': {e}. "
                f"Falling back to '{fallback_model_name}'."
            )
            tokenizer = AutoTokenizer.from_pretrained(fallback_model_name)
            model = AutoModelForSequenceClassification.from_pretrained(fallback_model_name)
            model.to("cuda" if self.device == 0 else "cpu")
            return tokenizer, model

    def _statistical_analysis(self, text: str) -> dict:
        """
        Performs statistical analysis to help identify AI-generated text.
        Calculates perplexity and burstiness.
        
        Args:
            text (str): The input text.

        Returns:
            dict: A dictionary containing statistical metrics.
        """
        tokens = text.split()
        if not tokens:
            return {"perplexity": 0, "burstiness": 0}

        # Perplexity (simplified as token distribution entropy)
        token_freq = np.unique(tokens, return_counts=True)[1]
        prob_dist = token_freq / len(tokens)
        perplexity = entropy(prob_dist, base=2)

        # Burstiness (variance in token distribution)
        burstiness = np.std(prob_dist)

        return {
            "perplexity": perplexity,
            "burstiness": float(burstiness)
        }

    def _model_based_llm_detection(self, text: str) -> dict:
        """
        Uses a transformer model to predict if the text is AI-generated.

        Args:
            text (str): The input text.

        Returns:
            dict: A dictionary containing the model's prediction and confidence.
        """
        if not self.llm_detection_pipeline:
            return {
                "error": "LLM detection model not available.",
                "ai_probability": 0.0,
                "confidence": 0.0
            }
        try:
            # Truncate text to fit model max length
            max_length = self.llm_detection_pipeline.model.config.max_position_embeddings - 2
            truncated_text = text[:max_length]

            result = self.llm_detection_pipeline(truncated_text)[0]
            
            # The label can be 'LABEL_1', 'AI', 'Human', etc. depending on the model
            # We assume the model returns a positive class for AI-generated text
            ai_probability = result['score'] if 'AI' in result['label'].upper() else (1 - result['score'])

            return {
                "model_name": self.llm_detection_pipeline.model.config._name_or_path,
                "ai_probability": ai_probability,
                "confidence": result['score']
            }
        except Exception as e:
            logger.error(f"Error during model-based LLM detection: {e}")
            return {"error": str(e), "ai_probability": 0.0, "confidence": 0.0}


    def analyze(self, text: str) -> dict:
        """
        Runs the full text analysis pipeline.

        Args:
            text (str): The input text to analyze.

        Returns:
            dict: A dictionary containing all analysis results.
        """
        if not text or not isinstance(text, str) or len(text.strip()) == 0:
            return {"error": "Input text is empty or invalid."}

        # --- 1. Misinformation Detection ---
        try:
            inputs = self.misinfo_tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to("cuda" if self.device == 0 else "cpu") for k, v in inputs.items()}
            with torch.no_grad():
                outputs = self.misinfo_model(**inputs)
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=-1).squeeze()
                
            # Assuming label 1 is 'misinformation'
            misinfo_prob = probabilities[1].item()
            confidence = torch.max(probabilities).item()
            
            misinfo_results = {
                "misinformation_probability": misinfo_prob,
                "confidence": confidence
            }
        except Exception as e:
            logger.error(f"Error during misinformation detection: {e}")
            misinfo_results = {"error": str(e)}

        # --- 2. LLM Origin Detection (Ensemble) ---
        stats_results = self._statistical_analysis(text)
        model_results = self._model_based_llm_detection(text)

        # Combine LLM detection results
        # Simple average between statistical hint (scaled) and model probability
        # This is a placeholder; a more sophisticated weighting could be used.
        model_prob = model_results.get("ai_probability", 0.0)
        
        # We can't directly map perplexity to probability, so we rely on the model primarily
        # The statistical results are returned for context
        final_llm_prob = model_prob

        llm_origin_results = {
            "llm_origin_probability": final_llm_prob,
            "model_confidence": model_results.get("confidence", 0.0),
            "statistical_analysis": stats_results,
            "model_based_analysis": model_results
        }

        return {
            "misinformation_detection": misinfo_results,
            "llm_origin_detection": llm_origin_results
        }


if __name__ == '__main__':
    print("Running TextAnalyzer smoke test...")
    analyzer = TextAnalyzer()

    # --- Test Case 1: Likely Human-Written (Factual News) ---
    human_text = (
        "The Eiffel Tower, a wrought-iron lattice tower on the Champ de Mars in Paris, France, "
        "is named after the engineer Gustave Eiffel, whose company designed and built the tower. "
        "Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially "
        "criticized by some of France's leading artists and intellectuals for its design."
    )
    print("\n--- Analyzing: Likely Human-Written Text ---")
    human_results = analyzer.analyze(human_text)
    import json
    print(json.dumps(human_results, indent=2))


    # --- Test Case 2: Potentially AI-Generated & Misinformation ---
    # This text is designed to have characteristics of AI text (e.g., generic phrasing)
    # and contains a factual error (moon cheese).
    ai_misinfo_text = (
        "In a startling recent discovery, scientists have conclusively found that the moon is "
        "composed of a rare, glowing cheese-like substance. This groundbreaking revelation, "
        "which challenges all previous astronomical knowledge, was made possible through "
        "advanced spectral analysis. The implications for space exploration are, "
        "needless to say, absolutely immense and profoundly significant."
    )
    print("\n--- Analyzing: Potentially AI-Generated & Misinformation Text ---")
    ai_results = analyzer.analyze(ai_misinfo_text)
    print(json.dumps(ai_results, indent=2))
    
    print("\nSmoke test completed.")